{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ee54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.applications.efficientnet as efn\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48830009",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_h = 224\n",
    "image_w = 224\n",
    "sample_size = 12000\n",
    "train_img_dir = '/kaggle/input/happy-whale-and-dolphin/train_images'\n",
    "test_img_dir = '/kaggle/input/happy-whale-and-dolphin/test_images'\n",
    "sub_path = '/kaggle/input/happy-whale-and-dolphin/sample_submission.csv'\n",
    "train_path = '/kaggle/input/happy-whale-and-dolphin/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "train_df.loc[train_df.species == \"bottlenose_dolpin\", \"species\"] = \"bottlenose_dolphin\"\n",
    "train_df.loc[train_df.species == \"kiler_whale\", \"species\"] = \"killer_whale\"\n",
    "train_df.loc[train_df.species == \"globis\", \"species\"] = \"short_finned_pilot_whale\"\n",
    "train_df.loc[train_df.species == \"pilot_whale\", \"species\"] = \"short_finned_pilot_whale\"\n",
    "train_df.loc[train_df.species == \"beluga\", \"species\"] = \"beluga_whale\"\n",
    "train_df.loc[train_df.species.str.contains(\"whale\")==True, \"label\"] = \"whale\"\n",
    "train_df.loc[train_df.species.str.contains(\"dolphin\")==True, \"label\"] = \"dolphin\"\n",
    "train_df_whale = train_df[train_df['label']=='whale']\n",
    "train_df_dolphin = train_df[train_df['label']=='dolphin']\n",
    "train_df_whale = shuffle(train_df_whale)\n",
    "train_df_dolphin = shuffle(train_df_dolphin)\n",
    "whale_F = train_df_whale.head(3000)\n",
    "dolphin_F = train_df_dolphin.head(3000)\n",
    "merged_train_f = pd.concat([whale_F,dolphin_F])\n",
    "merged_train_f = shuffle(merged_train_f)\n",
    "train_frame, test_frame = train_test_split(merged_train_f, test_size=0.15)\n",
    "print(len(train_df['species'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde217d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame['image_path'] = train_img_dir+'/'+train_frame['image']\n",
    "test_frame['image_path'] = train_img_dir+'/'+test_frame['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dae934",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_frame['label'].unique()))\n",
    "list_of_labels = train_frame['label'].unique()\n",
    "labels_to_neumeric = dict()\n",
    "for i in range(0,len(list_of_labels)):\n",
    "    labels_to_neumeric[list_of_labels[i]] = i\n",
    "print(labels_to_neumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df331d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_genration(train_df):#takes data frame input\n",
    "    train_image_list = []\n",
    "    for index,row in tqdm(train_df.iterrows()):\n",
    "    #     img = tf.io.read_file(row.image_path)\n",
    "    #     img = tf.image.decode_jpeg(img, channels=3)#can decode to another format \n",
    "    #     img = tf.image.resize(img, [128, 128])\n",
    "        img = tf.keras.preprocessing.image.load_img(row.image_path, target_size=(image_h,image_w,3), grayscale=False)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img/255\n",
    "        train_image_list.append(img)\n",
    "    X_train = np.array(train_image_list)\n",
    "    return X_train\n",
    "def test_data_generation(list_of_test_image_paths,f_t_num): #takes the image directory array and number of images\n",
    "    test_image_list = []\n",
    "    for i in tqdm(range(0,f_t_num)):\n",
    "        img = tf.keras.preprocessing.image.load_img(list_of_test_image_paths[i],\n",
    "                                                    target_size=(image_h,image_w,3), grayscale=False)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img/255\n",
    "        test_image_list.append(img)\n",
    "    X_T = np.array(test_image_list)\n",
    "    return X_T\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"acc\"])\n",
    "    plt.plot(hist.history[\"val_acc\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25337383",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_genration(train_frame)\n",
    "X_test = train_data_genration(test_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdb8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = tf.keras.applications.vgg19\n",
    "conv_model = vgg19.VGG19(weights='imagenet', include_top=False, input_shape=(image_h,image_w,3))\n",
    "for layer in conv_model.layers: \n",
    "    layer.trainable = False\n",
    "x = L.Flatten()(conv_model.output)\n",
    "x = L.Dense(100, activation='relu')(x)\n",
    "x = L.Dense(100, activation='relu')(x)\n",
    "x = L.Dense(100, activation='relu')(x)\n",
    "predictions = L.Dense(2, activation='softmax')(x)\n",
    "full_model = tf.keras.models.Model(inputs=conv_model.input, outputs=predictions)\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07815db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adamax(lr=0.001),\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba65862",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= train_frame['label'].values\n",
    "neumeric_label_list = []\n",
    "for label in labels:\n",
    "    neumeric_label_list.append(labels_to_neumeric[label])\n",
    "tr_lables = np.array(neumeric_label_list)\n",
    "print(tr_lables)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in tr_lables:\n",
    "    if(i==0):\n",
    "        count_0 = count_0+1\n",
    "    else:\n",
    "        count_1 = count_1+1\n",
    "print(count_0,\" \",count_1,\" \",len(tr_lables))\n",
    "tr_one_hot_label = to_categorical(tr_lables)\n",
    "print(tr_one_hot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b112b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wd, X_valid_wd, y_train_wd, y_valid_wd = train_test_split(X_train, tr_one_hot_label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history_l = full_model.fit(X_train_wd,y_train_wd, batch_size = 32,epochs=10,validation_data=(X_valid_wd,y_valid_wd)) #the batch size should be changed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(model_history_l)\n",
    "probabilities = full_model.predict(X_test)\n",
    "print(len(probabilities))\n",
    "lsit = np.argmax(probabilities,axis = 1)\n",
    "inverse_labels_to_neumeric = dict((v, k) for k, v in labels_to_neumeric.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "true_pre = 0\n",
    "false_pre = 0\n",
    "for index,row in test_frame.iterrows():\n",
    "#     print(row)\n",
    "    if(row.label == inverse_labels_to_neumeric[lsit[i]]):\n",
    "        true_pre = true_pre + 1\n",
    "    else:\n",
    "        false_pre = false_pre + 1\n",
    "#     print(\"actual Lable: \",row.label,' Predicted as:', inverse_labels_to_neumeric[lsit[i]])\n",
    "    i = i+1\n",
    "accuracy = true_pre/(true_pre + false_pre)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
