{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:07.802542Z",
     "iopub.status.busy": "2022-04-14T23:36:07.801817Z",
     "iopub.status.idle": "2022-04-14T23:36:13.218786Z",
     "shell.execute_reply": "2022-04-14T23:36:13.217967Z",
     "shell.execute_reply.started": "2022-04-14T23:36:07.802404Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved\n",
    "# as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of \n",
    "# the current session\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files\n",
    "# under the input directory\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.applications.efficientnet as efn\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:13.220669Z",
     "iopub.status.busy": "2022-04-14T23:36:13.220397Z",
     "iopub.status.idle": "2022-04-14T23:36:13.224571Z",
     "shell.execute_reply": "2022-04-14T23:36:13.223905Z",
     "shell.execute_reply.started": "2022-04-14T23:36:13.220631Z"
    }
   },
   "outputs": [],
   "source": [
    "image_h = 224\n",
    "image_w = 224\n",
    "sample_size = 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:25.181216Z",
     "iopub.status.busy": "2022-04-14T23:36:25.180908Z",
     "iopub.status.idle": "2022-04-14T23:36:27.414551Z",
     "shell.execute_reply": "2022-04-14T23:36:27.413819Z",
     "shell.execute_reply.started": "2022-04-14T23:36:25.181181Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    print(\"Device:\", tpu.master())\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n",
    "    strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:29.850536Z",
     "iopub.status.busy": "2022-04-14T23:36:29.849788Z",
     "iopub.status.idle": "2022-04-14T23:36:29.868028Z",
     "shell.execute_reply": "2022-04-14T23:36:29.867330Z",
     "shell.execute_reply.started": "2022-04-14T23:36:29.850481Z"
    }
   },
   "outputs": [],
   "source": [
    "img_augmentation = Sequential(\n",
    "    [\n",
    "        L.RandomRotation(factor=0.15),\n",
    "        L.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "        L.RandomFlip(),\n",
    "        L.RandomContrast(factor=0.1),\n",
    "    ],\n",
    "    name=\"img_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:36.701344Z",
     "iopub.status.busy": "2022-04-14T23:36:36.700512Z",
     "iopub.status.idle": "2022-04-14T23:36:36.706431Z",
     "shell.execute_reply": "2022-04-14T23:36:36.705381Z",
     "shell.execute_reply.started": "2022-04-14T23:36:36.701298Z"
    }
   },
   "outputs": [],
   "source": [
    "#print all file names\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename)) #end printing all file names\n",
    "\n",
    "train_img_dir = '/kaggle/input/happy-whale-and-dolphin/train_images'\n",
    "test_img_dir = '/kaggle/input/happy-whale-and-dolphin/test_images'\n",
    "sub_path = '/kaggle/input/happy-whale-and-dolphin/sample_submission.csv'\n",
    "train_path = '/kaggle/input/happy-whale-and-dolphin/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:40.886997Z",
     "iopub.status.busy": "2022-04-14T23:36:40.886263Z",
     "iopub.status.idle": "2022-04-14T23:36:41.100224Z",
     "shell.execute_reply": "2022-04-14T23:36:41.099419Z",
     "shell.execute_reply.started": "2022-04-14T23:36:40.886937Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "train_df.loc[train_df.species == \"bottlenose_dolpin\", \"species\"] = \"bottlenose_dolphin\"\n",
    "train_df.loc[train_df.species == \"kiler_whale\", \"species\"] = \"killer_whale\"\n",
    "train_df.loc[train_df.species == \"globis\", \"species\"] = \"short_finned_pilot_whale\"\n",
    "train_df.loc[train_df.species == \"pilot_whale\", \"species\"] = \"short_finned_pilot_whale\"\n",
    "train_df.loc[train_df.species == \"beluga\", \"species\"] = \"beluga_whale\"\n",
    "train_df.loc[train_df.species.str.contains(\"whale\")==True, \"label\"] = \"whale\"\n",
    "train_df.loc[train_df.species.str.contains(\"dolphin\")==True, \"label\"] = \"dolphin\"\n",
    "#the following can also be used however pd.np is deprecated in the future version \n",
    "# train_df['label'] = pd.np.where(train_df.species.str.contains(\"whale\"), \"whale\",\n",
    "#                    pd.np.where(train_df.species.str.contains(\"dolphin\"), \"dolphin\",\"task\"))\n",
    "# train_df_all = train_df\n",
    "train_df = train_df.sample(frac=.10).reset_index(drop=True)\n",
    "\n",
    "train_df, test_df = train_test_split(train_df, test_size=0.2)\n",
    "# train_df = train_df.head(sample_size)\n",
    "print(len(train_df['species'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:45.979226Z",
     "iopub.status.busy": "2022-04-14T23:36:45.978693Z",
     "iopub.status.idle": "2022-04-14T23:36:45.986790Z",
     "shell.execute_reply": "2022-04-14T23:36:45.985872Z",
     "shell.execute_reply.started": "2022-04-14T23:36:45.979184Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add image url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:49.446444Z",
     "iopub.status.busy": "2022-04-14T23:36:49.445663Z",
     "iopub.status.idle": "2022-04-14T23:36:50.004603Z",
     "shell.execute_reply": "2022-04-14T23:36:50.003779Z",
     "shell.execute_reply.started": "2022-04-14T23:36:49.446404Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['image_path'] = train_img_dir+'/'+train_df['image']\n",
    "list_of_test_image_paths = glob.glob(test_img_dir+'/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Label to neumeric lableing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:54.437132Z",
     "iopub.status.busy": "2022-04-14T23:36:54.436353Z",
     "iopub.status.idle": "2022-04-14T23:36:54.445839Z",
     "shell.execute_reply": "2022-04-14T23:36:54.444913Z",
     "shell.execute_reply.started": "2022-04-14T23:36:54.437090Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_df['species'].unique()))\n",
    "list_of_species = train_df['species'].unique()\n",
    "print(train_df['species'].unique())\n",
    "species_to_neumeric = dict()\n",
    "for i in range(0,len(list_of_species)):\n",
    "    species_to_neumeric[list_of_species[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:36:58.079228Z",
     "iopub.status.busy": "2022-04-14T23:36:58.078366Z",
     "iopub.status.idle": "2022-04-14T23:36:58.087561Z",
     "shell.execute_reply": "2022-04-14T23:36:58.086636Z",
     "shell.execute_reply.started": "2022-04-14T23:36:58.079180Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_df['individual_id'].unique()))\n",
    "list_of_ids = train_df['individual_id'].unique()\n",
    "ids_to_neumeric = dict()\n",
    "for i in range(0,len(list_of_ids)):\n",
    "    ids_to_neumeric[list_of_ids[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:37:01.769678Z",
     "iopub.status.busy": "2022-04-14T23:37:01.769040Z",
     "iopub.status.idle": "2022-04-14T23:37:01.776588Z",
     "shell.execute_reply": "2022-04-14T23:37:01.775805Z",
     "shell.execute_reply.started": "2022-04-14T23:37:01.769635Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_df['label'].unique()))\n",
    "list_of_labels = train_df['label'].unique()\n",
    "labels_to_neumeric = dict()\n",
    "for i in range(0,len(list_of_labels)):\n",
    "    labels_to_neumeric[list_of_labels[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Pre-Processing\n",
    "1. Resizing the image\n",
    "2. Converting the colored image to greyscale image may be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:37:05.081361Z",
     "iopub.status.busy": "2022-04-14T23:37:05.081087Z",
     "iopub.status.idle": "2022-04-14T23:37:05.086752Z",
     "shell.execute_reply": "2022-04-14T23:37:05.085750Z",
     "shell.execute_reply.started": "2022-04-14T23:37:05.081330Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to resize each image \n",
    "def resize_images(path,image_w,image_h):#n_w =new width n_h = new_height\n",
    "    img = tf.io.read_file(path)\n",
    "#     img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)#can decode to another format \n",
    "    img = tf.image.resize(img, [image_w, image_h])\n",
    "    return img\n",
    "#concern After resizing it what we can do should we store the image to a new directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate the images and resize and store in a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test data generation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:37:09.576630Z",
     "iopub.status.busy": "2022-04-14T23:37:09.575888Z",
     "iopub.status.idle": "2022-04-14T23:37:09.591050Z",
     "shell.execute_reply": "2022-04-14T23:37:09.590303Z",
     "shell.execute_reply.started": "2022-04-14T23:37:09.576586Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def train_data_genration(train_df):#takes data frame input\n",
    "    train_image_list = []\n",
    "    for index,row in tqdm(train_df.iterrows()):\n",
    "    #     img = tf.io.read_file(row.image_path)\n",
    "    #     img = tf.image.decode_jpeg(img, channels=3)#can decode to another format \n",
    "    #     img = tf.image.resize(img, [128, 128])\n",
    "        img = tf.keras.preprocessing.image.load_img(row.image_path, target_size=(image_h,image_w,3), grayscale=False)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img/255\n",
    "        train_image_list.append(img)\n",
    "    X_train = np.array(train_image_list)\n",
    "    return X_train\n",
    "def test_data_generation(list_of_test_image_paths,f_t_num): #takes the image directory array and number of images\n",
    "    test_image_list = []\n",
    "    for i in tqdm(range(0,f_t_num)):\n",
    "        img = tf.keras.preprocessing.image.load_img(list_of_test_image_paths[i],\n",
    "                                                    target_size=(image_h,image_w,3), grayscale=False)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img/255\n",
    "        test_image_list.append(img)\n",
    "    X_T = np.array(test_image_list)\n",
    "    return X_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:37:13.928822Z",
     "iopub.status.busy": "2022-04-14T23:37:13.927362Z",
     "iopub.status.idle": "2022-04-14T23:42:57.893679Z",
     "shell.execute_reply": "2022-04-14T23:42:57.892805Z",
     "shell.execute_reply.started": "2022-04-14T23:37:13.928766Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_data_genration(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:42:57.895835Z",
     "iopub.status.busy": "2022-04-14T23:42:57.895455Z",
     "iopub.status.idle": "2022-04-14T23:44:23.717787Z",
     "shell.execute_reply": "2022-04-14T23:44:23.716920Z",
     "shell.execute_reply.started": "2022-04-14T23:42:57.895794Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df['image_path'] = train_img_dir+'/'+test_df['image']\n",
    "\n",
    "X_test = train_data_genration(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.386120Z",
     "iopub.status.idle": "2022-04-14T22:32:30.386559Z",
     "shell.execute_reply": "2022-04-14T22:32:30.386334Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.386312Z"
    }
   },
   "outputs": [],
   "source": [
    "t_num = len(list_of_test_image_paths) #27956 number of images \n",
    "f_t_num = int(t_num/1000) # trying to test for small number \n",
    "# for t_path in list_of_test_image_paths:#when all images will be tested \n",
    "temporary_list = sample(list_of_test_image_paths,f_t_num)\n",
    "X_test = test_data_generation(temporary_list,f_t_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indentifying whale and dolphin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:44:23.719785Z",
     "iopub.status.busy": "2022-04-14T23:44:23.719477Z",
     "iopub.status.idle": "2022-04-14T23:44:23.730005Z",
     "shell.execute_reply": "2022-04-14T23:44:23.729288Z",
     "shell.execute_reply.started": "2022-04-14T23:44:23.719746Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_binary_classification_model():\n",
    "    inputs = L.Input(shape=(image_h, image_w, 3))\n",
    "    x = img_augmentation(inputs) #how the input sensor is prepared\n",
    "    model = efn.EfficientNetB0(include_top=False, input_tensor=x, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = L.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = L.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.1\n",
    "    x = L.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = L.Dense(2, activation=\"sigmoid\", name=\"pred\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def plot_hist(hist):\n",
    "    plt.plot(hist.history[\"accuracy\"])\n",
    "    plt.plot(hist.history[\"val_accuracy\"])\n",
    "    plt.title(\"model accuracy\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.389798Z",
     "iopub.status.idle": "2022-04-14T22:32:30.390203Z",
     "shell.execute_reply": "2022-04-14T22:32:30.390007Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.389985Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [image_h, image_w]\n",
    "dense_layer_size = len(labels_to_neumeric.values())\n",
    "model_l = tf.keras.Sequential([efn.EfficientNetB0(input_shape=(*IMAGE_SIZE, 3),\n",
    "                                                weights='imagenet',\n",
    "                                                include_top=False,classes=labels_to_neumeric.values()),\n",
    "                             L.GlobalAveragePooling2D(),\n",
    "                            L.BatchNormalization(),\n",
    "                               L.Dropout(0.20, name=\"top_dropout\"),\n",
    "                             L.Flatten(),\n",
    "                             L.Dense(512, activation='relu'),\n",
    "                             L.Dense(dense_layer_size, activation='sigmoid')])\n",
    "model_l.compile(optimizer='adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model_l.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model For Identifying species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.391618Z",
     "iopub.status.idle": "2022-04-14T22:32:30.392016Z",
     "shell.execute_reply": "2022-04-14T22:32:30.391823Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.391802Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [image_h, image_w]\n",
    "dense_layer_size = len(species_to_neumeric.values())\n",
    "model = tf.keras.Sequential([efn.EfficientNetB0(input_shape=(*IMAGE_SIZE, 3),\n",
    "                                                weights='imagenet',\n",
    "                                                include_top=False,classes=species_to_neumeric.values()),\n",
    "                             L.GlobalAveragePooling2D(),\n",
    "                             L.Flatten(),\n",
    "                             L.Dense(512, activation='relu'),\n",
    "                             L.Dense(dense_layer_size, activation='softmax')])\n",
    "model.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for individual Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.393332Z",
     "iopub.status.idle": "2022-04-14T22:32:30.394090Z",
     "shell.execute_reply": "2022-04-14T22:32:30.393874Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.393839Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [image_h, image_w]\n",
    "dense_layer_size = len(ids_to_neumeric.values())\n",
    "model_ind = tf.keras.Sequential([efn.EfficientNetB0(input_shape=(*IMAGE_SIZE, 3),\n",
    "                                                weights='imagenet',\n",
    "                                                include_top=False,classes=ids_to_neumeric.values()),\n",
    "                             L.GlobalAveragePooling2D(),\n",
    "                             L.Flatten(),\n",
    "                             L.Dense(512, activation='relu'),\n",
    "                             L.Dense(dense_layer_size, activation='softmax')])\n",
    "model_ind.compile(optimizer='adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model_ind.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neumerical Label to catergorical for whale and dolphin detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:44:23.732577Z",
     "iopub.status.busy": "2022-04-14T23:44:23.732284Z",
     "iopub.status.idle": "2022-04-14T23:44:23.750426Z",
     "shell.execute_reply": "2022-04-14T23:44:23.749697Z",
     "shell.execute_reply.started": "2022-04-14T23:44:23.732542Z"
    }
   },
   "outputs": [],
   "source": [
    "labels= train_df['label'].values\n",
    "neumeric_label_list = []\n",
    "for label in labels:\n",
    "    neumeric_label_list.append(labels_to_neumeric[label])\n",
    "# label = []\n",
    "# for i in y:\n",
    "#     if i=='whale':\n",
    "#         label.append(1)\n",
    "#     else:\n",
    "#         label.append(0)\n",
    "\n",
    "# tr_labes = np.array(label)\n",
    "tr_lables = np.array(neumeric_label_list)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for i in tr_lables:\n",
    "    if(i==0):\n",
    "        count_0 = count_0+1\n",
    "    else:\n",
    "        count_1 = count_1+1\n",
    "print(count_0,\" \",count_1,\" \",len(tr_lables))\n",
    "tr_one_hot_label = to_categorical(tr_lables)\n",
    "print(tr_one_hot_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split for training whale and dolphin detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:44:23.752315Z",
     "iopub.status.busy": "2022-04-14T23:44:23.752049Z",
     "iopub.status.idle": "2022-04-14T23:44:24.566952Z",
     "shell.execute_reply": "2022-04-14T23:44:24.565786Z",
     "shell.execute_reply.started": "2022-04-14T23:44:23.752281Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_wd, X_valid_wd, y_train_wd, y_valid_wd = train_test_split(X_train, tr_one_hot_label, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T23:44:24.569545Z",
     "iopub.status.busy": "2022-04-14T23:44:24.569145Z",
     "iopub.status.idle": "2022-04-15T00:35:22.959916Z",
     "shell.execute_reply": "2022-04-15T00:35:22.959127Z",
     "shell.execute_reply.started": "2022-04-14T23:44:24.569481Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = build_binary_classification_model()\n",
    "epochs = 500  # @param {type: \"slider\", min:8, max:80}\n",
    "hist = model.fit(X_train_wd, y_train_wd, batch_size = 32, epochs=epochs, validation_data=(X_valid_wd,y_valid_wd), verbose=2)\n",
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.403059Z",
     "iopub.status.idle": "2022-04-14T22:32:30.403482Z",
     "shell.execute_reply": "2022-04-14T22:32:30.403272Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.403250Z"
    }
   },
   "outputs": [],
   "source": [
    "model_history_l = model_l.fit(X_train_wd, y_train_wd, batch_size = 32,epochs=10,validation_data=(X_valid_wd, y_valid_wd)) #the batch size should be changed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T22:49:53.207846Z",
     "iopub.status.busy": "2022-04-14T22:49:53.207587Z",
     "iopub.status.idle": "2022-04-14T22:49:57.420962Z",
     "shell.execute_reply": "2022-04-14T22:49:57.420193Z",
     "shell.execute_reply.started": "2022-04-14T22:49:53.207817Z"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T22:50:05.699545Z",
     "iopub.status.busy": "2022-04-14T22:50:05.699284Z",
     "iopub.status.idle": "2022-04-14T22:50:05.704948Z",
     "shell.execute_reply": "2022-04-14T22:50:05.704259Z",
     "shell.execute_reply.started": "2022-04-14T22:50:05.699516Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(probabilities))\n",
    "lsit = np.argmax(probabilities,axis = 1)\n",
    "inverse_labels_to_neumeric = dict((v, k) for k, v in labels_to_neumeric.items())\n",
    "for i in lsit:\n",
    "    pass\n",
    "#     print(inverse_labels_to_neumeric[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T22:53:53.869561Z",
     "iopub.status.busy": "2022-04-14T22:53:53.869282Z",
     "iopub.status.idle": "2022-04-14T22:53:54.212674Z",
     "shell.execute_reply": "2022-04-14T22:53:54.211959Z",
     "shell.execute_reply.started": "2022-04-14T22:53:53.869531Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for index,row in test_df.iterrows():\n",
    "#     print(row)\n",
    "    print(\"actual Lable: \",row.label,' Predicted as:', inverse_labels_to_neumeric[lsit[i]])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.408424Z",
     "iopub.status.idle": "2022-04-14T22:32:30.409018Z",
     "shell.execute_reply": "2022-04-14T22:32:30.408792Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.408769Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,f_t_num):\n",
    "    re_img = resize_images(temporary_list[i],image_w,image_h)\n",
    "    image = tf.cast(re_img, np.uint8)\n",
    "    print(temporary_list[i],' as', inverse_labels_to_neumeric[lsit[i]])\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neumerical Label to Categorical for species Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.410300Z",
     "iopub.status.idle": "2022-04-14T22:32:30.410856Z",
     "shell.execute_reply": "2022-04-14T22:32:30.410660Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.410635Z"
    }
   },
   "outputs": [],
   "source": [
    "species_as_label = train_df['species'].values\n",
    "species_label_list = []\n",
    "for species in species_as_label:\n",
    "    species_label_list.append(species_to_neumeric[species])\n",
    "# label = []\n",
    "# for i in y:\n",
    "#     if i=='whale':\n",
    "#         label.append(1)\n",
    "#     else:\n",
    "#         label.append(0)\n",
    "\n",
    "# tr_labes = np.array(label)\n",
    "tr_sp_lables = np.array(species_label_list)\n",
    "tr_sp_one_hot_label = to_categorical(tr_sp_lables)\n",
    "len(tr_sp_one_hot_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neumerical Label to Categorical for Individual Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.411913Z",
     "iopub.status.idle": "2022-04-14T22:32:30.412464Z",
     "shell.execute_reply": "2022-04-14T22:32:30.412248Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.412223Z"
    }
   },
   "outputs": [],
   "source": [
    "individual_as_label = train_df['individual_id'].values\n",
    "individual_label_list = []\n",
    "for individual in individual_as_label:\n",
    "    individual_label_list.append(ids_to_neumeric[individual])\n",
    "# label = []\n",
    "# for i in y:\n",
    "#     if i=='whale':\n",
    "#         label.append(1)\n",
    "#     else:\n",
    "#         label.append(0)\n",
    "\n",
    "# tr_labes = np.array(label)\n",
    "tr_id_lables = np.array(individual_label_list)\n",
    "tr_id_one_hot_label = to_categorical(tr_id_lables)\n",
    "tr_id_one_hot_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into Train and validation set  for Species detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.413505Z",
     "iopub.status.idle": "2022-04-14T22:32:30.414056Z",
     "shell.execute_reply": "2022-04-14T22:32:30.413853Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.413830Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_SD, X_valid_SD, y_train_SD, y_valid_SD = train_test_split(X_train, tr_sp_one_hot_label, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into Train and validation set  for individual detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.415107Z",
     "iopub.status.idle": "2022-04-14T22:32:30.415664Z",
     "shell.execute_reply": "2022-04-14T22:32:30.415444Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.415421Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ID, X_valid_ID, y_train_ID, y_valid_ID = train_test_split(X_train, tr_id_one_hot_label, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model for Species detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.416735Z",
     "iopub.status.idle": "2022-04-14T22:32:30.417278Z",
     "shell.execute_reply": "2022-04-14T22:32:30.417075Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.417050Z"
    }
   },
   "outputs": [],
   "source": [
    "model_history = model.fit(X_train_SD, y_train_SD, batch_size = 32,epochs=10,validation_data=(X_valid_SD, y_valid_SD)) #the batch size should be changed \n",
    "#according to the total input size if train image is very small and batch size is almost equal to train image numbers it will\n",
    "#return OOM -->out of memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.418331Z",
     "iopub.status.idle": "2022-04-14T22:32:30.418896Z",
     "shell.execute_reply": "2022-04-14T22:32:30.418686Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.418664Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('whale_detection_model.h5') \n",
    "model_h = tf.keras.models.load_model('whale_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.419954Z",
     "iopub.status.idle": "2022-04-14T22:32:30.420507Z",
     "shell.execute_reply": "2022-04-14T22:32:30.420286Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.420261Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(model.history['val_accuracy'])\n",
    "# summarize history for accuracy\n",
    "plt.plot(model_history.history['accuracy'])\n",
    "plt.plot(model_history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.421598Z",
     "iopub.status.idle": "2022-04-14T22:32:30.422133Z",
     "shell.execute_reply": "2022-04-14T22:32:30.421930Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.421904Z"
    }
   },
   "outputs": [],
   "source": [
    "t_num = len(list_of_test_image_paths) #27956 number of images \n",
    "f_t_num = int(t_num/100) # trying to test for small number \n",
    "# for t_path in list_of_test_image_paths:#when all images will be tested \n",
    "X_test = test_data_generation(list_of_test_image_paths,f_t_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.423203Z",
     "iopub.status.idle": "2022-04-14T22:32:30.423760Z",
     "shell.execute_reply": "2022-04-14T22:32:30.423562Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.423525Z"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.424815Z",
     "iopub.status.idle": "2022-04-14T22:32:30.425361Z",
     "shell.execute_reply": "2022-04-14T22:32:30.425151Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.425127Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(probabilities))\n",
    "lsit = np.argmax(probabilities,axis = 1)\n",
    "inverse_species_to_neumeric = dict((v, k) for k, v in species_to_neumeric.items())\n",
    "for i in lsit:\n",
    "    pass\n",
    "#     print(inverse_species_to_neumeric[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.426427Z",
     "iopub.status.idle": "2022-04-14T22:32:30.426980Z",
     "shell.execute_reply": "2022-04-14T22:32:30.426777Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.426750Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,f_t_num):\n",
    "    re_img = resize_images(temporary_list[i],image_w,image_h)\n",
    "    image = tf.cast(re_img, np.uint8)\n",
    "    print(list_of_test_image_paths[i], ' as', inverse_species_to_neumeric[lsit[i]])\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model for Individual detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.428036Z",
     "iopub.status.idle": "2022-04-14T22:32:30.428593Z",
     "shell.execute_reply": "2022-04-14T22:32:30.428382Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.428346Z"
    }
   },
   "outputs": [],
   "source": [
    "model_history_ind = model_ind.fit(X_train_ID, y_train_ID, batch_size = 32,epochs=100,validation_data=(X_valid_ID, y_valid_ID)) #the batch size should be changed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.429636Z",
     "iopub.status.idle": "2022-04-14T22:32:30.430171Z",
     "shell.execute_reply": "2022-04-14T22:32:30.429970Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.429945Z"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = model_ind.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.431227Z",
     "iopub.status.idle": "2022-04-14T22:32:30.431785Z",
     "shell.execute_reply": "2022-04-14T22:32:30.431585Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.431559Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(probabilities))\n",
    "lsit = np.argmax(probabilities,axis = 1)\n",
    "inverse_ids_to_neumeric = dict((v, k) for k, v in ids_to_neumeric.items())\n",
    "for i in lsit:\n",
    "    print(inverse_ids_to_neumeric[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.432853Z",
     "iopub.status.idle": "2022-04-14T22:32:30.433401Z",
     "shell.execute_reply": "2022-04-14T22:32:30.433190Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.433166Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df[train_df['individual_id'] == \"3cf81d69cc5911\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.434449Z",
     "iopub.status.idle": "2022-04-14T22:32:30.435005Z",
     "shell.execute_reply": "2022-04-14T22:32:30.434803Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.434778Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(0,f_t_num):\n",
    "    re_img = resize_images(list_of_test_image_paths[i],image_w,image_h)\n",
    "    image = tf.cast(re_img, np.uint8)\n",
    "    print(list_of_test_image_paths[i])\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating the data for the individual training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.436059Z",
     "iopub.status.idle": "2022-04-14T22:32:30.436622Z",
     "shell.execute_reply": "2022-04-14T22:32:30.436403Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.436379Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df_all['image_path'] = train_img_dir+'/'+train_df_all['image']\n",
    "# train_data_group_by_id = train_df_all.groupby('individual_id')\n",
    "train_data_group_by_id = train_df.groupby('individual_id')\n",
    "train_groups_array = dict()\n",
    "val_groups_array = []\n",
    "for g_index,group in train_data_group_by_id:\n",
    "    if(len(group) < 5):\n",
    "        val_groups_array.append(group)\n",
    "    else:\n",
    "        train_groups_array[g_index] = group\n",
    "# train_group_data_frame = pd.concat(train_groups_array) \n",
    "val_group_data_frame = pd.concat(val_groups_array)\n",
    "print(len(val_group_data_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features by RESNET50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.437672Z",
     "iopub.status.idle": "2022-04-14T22:32:30.438205Z",
     "shell.execute_reply": "2022-04-14T22:32:30.438005Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.437980Z"
    }
   },
   "outputs": [],
   "source": [
    "#transfor the image to numpy array for training \n",
    "#then fit a model per each individual\n",
    "def extract_resnet(X,image_h,image_w):  \n",
    "    # X : images numpy array\n",
    "    resnet_model = ResNet50(input_shape=(image_h, image_w, 3), weights='imagenet', include_top=False)  # Since top layer is the fc layer used for predictions\n",
    "    features_array = resnet_model.predict(X)\n",
    "    return features_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incomplete code for individual model and prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.439291Z",
     "iopub.status.idle": "2022-04-14T22:32:30.439845Z",
     "shell.execute_reply": "2022-04-14T22:32:30.439648Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.439624Z"
    }
   },
   "outputs": [],
   "source": [
    "model_array = dict()\n",
    "for key in train_groups_array.keys():\n",
    "    X_train_temp = train_data_genration(train_groups_array[key])\n",
    "    X_train_temp_features = extract_resnet(X_train_temp,image_h,image_w)\n",
    "    X_train_temp_features = X_train_temp_features.reshape(len(train_groups_array[key]),\n",
    "                                                          int(X_train_temp_features.size/len(train_groups_array[key])))\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train_temp_features)\n",
    "    X_train = ss.transform(X_train_temp_features)\n",
    "#     X_val = ss.transform(X_val)\n",
    "    # Take PCA to reduce feature space dimensionality\n",
    "    pca = PCA(n_components=len(train_groups_array[key]), whiten=True)# as the componant number min(number of sample, feature)\n",
    "    pca = pca.fit(X_train)\n",
    "#     print('Explained variance percentage = %0.2f' % sum(pca.explained_variance_ratio_))\n",
    "    X_train = pca.transform(X_train)\n",
    "#     X_val = pca.transform(X_val)\n",
    "    oc_svm_clf = svm.OneClassSVM(gamma=0.001, kernel='rbf', nu=0.08)  # Obtained using grid search\n",
    "    oc_svm_clf.fit(X_train)\n",
    "    model_array[key] = oc_svm_clf\n",
    "    test_image_batches = [list_of_test_image_paths[i:i + len(train_groups_array[key])] for i in range(0, len(list_of_test_image_paths),\n",
    "                                                                                 len(train_groups_array[key]))]\n",
    "    for batch in test_image_batches:\n",
    "        X_test_temp = test_data_generation(batch,len(batch))\n",
    "        X_test_temp_features = extract_resnet(X_test_temp,image_h,image_w)\n",
    "        X_test_temp_features = X_test_temp_features.reshape(len(train_groups_array[key]),\n",
    "                                                          int(X_test_temp_features.size/len(train_groups_array[key])))\n",
    "        X_test = ss.transform(X_test_temp_features)\n",
    "        X_test = pca.transform(X_test)\n",
    "        oc_svm_preds = oc_svm_clf.predict(X_test)\n",
    "#     if_preds = if_clf.predict(X_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried other things not required now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-14T22:32:30.440920Z",
     "iopub.status.idle": "2022-04-14T22:32:30.441492Z",
     "shell.execute_reply": "2022-04-14T22:32:30.441259Z",
     "shell.execute_reply.started": "2022-04-14T22:32:30.441235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply standard scaler to output from resnet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import resnet50\n",
    "\n",
    "for i in range(0,100):\n",
    "    test_image = load_img(list_of_test_image_paths[i], target_size = (image_h, image_w)) \n",
    "    test_np_image = img_to_array(test_image) \n",
    "    test_image_batch = np.expand_dims(test_np_image, axis = 0) \n",
    "    test_processed_image = resnet50.preprocess_input(test_image_batch.copy())\n",
    "#     ss = StandardScaler()\n",
    "#     test_processed_image = ss.transform(test_processed_image)\n",
    "    # Take PCA to reduce feature space dimensionality\n",
    "#     pca = PCA(n_components=512, whiten=True)\n",
    "#     pca = pca.fit(test_processed_image)\n",
    "#     test_processed_image = pca.transform(test_processed_image)\n",
    "    print(test_processed_image.shape)\n",
    "    test_processed_image = test_processed_image.reshape(1,image_h*image_w*3)\n",
    "\n",
    "    for key in model_array.keys():\n",
    "        oc_svm_preds = oc_svm_clf.predict(test_processed_image)\n",
    "        print(oc_svm_preds)\n",
    "    break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
